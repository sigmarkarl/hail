.. _sec-tutorial:

========
Tutorial
========

This tutorial goes through the basic concepts of Pipeline with examples.


Import
------

Pipeline is located inside the `hailtop` module, which can be installed
as described in the :ref:`Getting Started <sec-getting_started>` section.

.. code-block:: python

    >>> import hailtop.pipeline as hp


.. _f-strings:

f-strings
---------

f-strings were added to Python in version 3.6 and are denoted by the 'f' character
before a string literal. When creating the string, Python evaluates any expressions
in single curly braces `{...}` using the current variable scope. When Python compiles
the example below, the string 'Alice' is substituted for `{name}` because the variable
`name` is set to 'Alice' in the line above.

.. code-block:: python

    >>> name = 'Alice'
    >>> print(f'hello {name}')
    hello Alice

You can put any arbitrary Python code inside the curly braces and Python will evaluate
the expression correctly. For example, below we evaluate `x + 1` first before compiling
the string. Therefore, we get 'x = 6' as the resulting string.

.. code-block:: python

    >>> x = 5
    >>> print(f'x = {x + 1}')
    x = 6

To use an f-string and output a single curly brace in the output string, escape the curly
brace by duplicating the character. For example, `{` becomes `{{` in the string definition,
but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`.

.. code-block:: python

    >>> x = 5
    >>> print(f'x = {{x + 1}} plus {x}')
    x = {x + 1} plus 5

To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_.

Hello World
-----------

A :class:`.Pipeline` consists of a set of :class:`.Task` to execute. There can be
an arbitrary number of tasks in the pipeline that are executed in order of their dependencies.
A dependency between two tasks states that the dependent task should not run until
the previous task completes. Thus, under the covers a pipeline is a directed acyclic graph (DAG)
of tasks.

In the example below, we have defined a :class:`.Pipeline` `p` with the name 'hello'.
We use the method :meth:`.Pipeline.new_task` to create a task object which we call `t` and then
use the method :meth:`.Task.command` to tell Pipeline that we want to execute `echo "hello world"`.
However, at this point, Pipeline hasn't actually run the task to print "hello world". All we have
done is specified the tasks and the order in which they should be run. To actually execute the
Pipeline, we call :meth:`.Pipeline.run`. The `name` arguments to both :class:`.Pipeline` and
:class:`.Task` are used in the :ref:`Batch Service UI <sec-batch-service>`.


.. code-block:: python

    >>> p = hp.Pipeline(name='hello')
    >>> t = p.new_task(name='t1')
    >>> t.command('echo "hello world"')
    >>> p.run()


Now that we know how to create a pipeline with a single task, we call :meth:`.Pipeline.new_task`
twice to create two tasks `s` and `t` which both will print a variant of hello world to stdout.
Calling `p.run()` executes the pipeline. By default, pipelines are executed by the :class:`.LocalBackend`
which runs tasks on your local computer. Therefore, even though these tasks can be run in parallel,
they are still run sequentially. However, if pipelines are executed by the :class:`.BatchBackend`
using the :ref:`Batch Service <sec-batch-service>`, then `s` and `t` can be run in parallel as
there exist no dependencies between them.

.. code-block:: python

    >>> p = hp.Pipeline(name='hello-parallel')
    >>> s = p.new_task(name='t1')
    >>> s.command('echo "hello world 1"')
    >>> t = p.new_task(name='t2')
    >>> t.command('echo "hello world 2"')
    >>> p.run()

To create a dependency between `s` and `t`, we use the method :class:`.Task.depends_on` to
explicitly state that `t` depends on `s`. In both the :class:`.LocalBackend` and
:class:`.BatchBackend`, `s` will always run before `t`.


.. code-block:: python

    >>> p = hp.Pipeline(name='hello-serial')
    >>> s = p.new_task(name='t1')
    >>> s.command('echo "hello world 1"')
    >>> t = p.new_task(name='t2')
    >>> t.command('echo "hello world 2"')
    >>> t.depends_on(s)
    >>> p.run()


.. _file-dependencies:

File Dependencies
-----------------

So far we have created pipelines with two tasks where the dependencies between
them were declared explicitly. However, in many pipelines, we want to have a file
generated by one task be the input to a downstream task. Pipeline has a mechanism
for tracking file outputs and then inferring task dependencies from the usage of
those files.

In the example below, we have specified two tasks: `s` and `t`. `s` prints
"hello world" as in previous examples. However, instead of printing to stdout,
this time `s` redirects the output to a temporary file defined by `s.ofile`.
`s.ofile` is a Python object of type :class:`.TaskResourceFile` that was created
on the fly when we accessed an attribute of a :class:`.Task` that does not already
exist. Any time we access the attribute again (in this example `ofile`), we get the
same :class:`.TaskResourceFile` that was previously created. However, be aware that
you cannot use an existing method or property name of :class:`.Task` objects such
as :meth:`.Task.command` or :meth:`.Task.image`.

Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so
when Python interpolates the :ref:`f-string <f-strings>`, it replaced the
:class:`.TaskResourceFile` object with an actual file path into the command for `s`.
We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.
`s.ofile` is the same temporary file that was created in the command for `t`. Therefore,
pipeline deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.
In both the :class:`.LocalBackend` and :class:`.BatchBackend`, `s` will always run before `t`.

.. code-block:: python

    >>> p = hp.Pipeline(name='hello-serial')
    >>> s = p.new_task(name='t1')
    >>> s.command(f'echo "hello world" > {s.ofile}')
    >>> t = p.new_task(name='t2')
    >>> t.command(f'cat {s.ofile}')
    >>> p.run()


Scatter / Gather
----------------

Pipeline is implemented in Python making it easy to use for loops
to create more complicated dependency graphs between tasks. We define a scatter
to be a pipeline that runs the same command with varying input parameters and a gather
is a final task or "sink" that waits for all of the tasks in the scatter to be complete
before executing.

In the example below, we use a for loop to create a task for each one of
'Alice', 'Bob', and 'Dan' that prints the name of the user programatically
thereby scattering the echo command over users.



.. code-block:: python

    >>> p = hp.Pipeline(name='scatter')
    >>> for name in ['Alice', 'Bob', 'Dan']:
    ...     t = p.new_task(name=name)
    ...     t.command(f'echo "hello {name}"')
    >>> p.run()

In the previous example, we did not assign the tasks we created for each
user to a unique variable name and instead named it `t` each time in the
for loop. However, if we want to add a final gather task (`sink`) that depends on the
completion of all user tasks, then we need to keep track of all of the user
tasks so we can use the :meth:`.Task.depends_on` method to explicitly link
the `sink` task to be dependent on the user tasks, which are stored in the
`tasks` array. The single asterisk before `tasks` is used in Python to have
all elements in the array be treated as separate input arguments to the function,
in this case :meth:`.Task.depends_on`.

.. image:: _static/images/dags/dags.005.png

.. code-block:: python

    >>> p = hp.Pipeline(name='scatter-gather-1')
    >>> tasks = []
    >>> for name in ['Alice', 'Bob', 'Dan']:
    ...     t = p.new_task(name=name)
    ...     t.command(f'echo "hello {name}"')
    ...     tasks.append(t)
    >>> sink = p.new_task(name='sink')
    >>> sink.command(f'echo "I wait for everyone"')
    >>> sink.depends_on(*tasks)
    >>> p.run()

Now that we know how to create a `sink` task that depends on an arbitrary
number of tasks, we want to have the outputs of each of the per-user tasks
be implicit file dependencies in the `sink` task (see the section on
:ref:`file dependencies <file-dependencies>`). The changes from the previous
example to make this happen are each task `t` uses an :ref:`f-string <f-strings>`
to create a temporary output file `t.ofile` where the output to echo is redirected.
We then use all of the output files in the `sink` command by creating a string
with the temporary output file names for each task. A :class:`.TaskResourceFile`
is a Pipeline-specific object that inherits from `str`. Therefore, you can use
:class:`.TaskResourceFile` as if they were strings, which we do with the `join`
command for strings.

.. image:: _static/images/dags/dags.006.png

.. code-block:: python

    >>> p = hp.Pipeline(name='scatter-gather-2')
    >>> tasks = []
    >>> for name in ['Alice', 'Bob', 'Dan']:
    ...     t = p.new_task(name=name)
    ...     t.command(f'echo "hello {name}" > {t.ofile}')
    ...     tasks.append(t)
    >>> sink = p.new_task(name='sink')
    >>> sink.command('cat {}'.format(' '.join([t.ofile for t in tasks])))
    >>> p.run()


Nested Scatters
---------------

We can also create a nested scatter where we do a series of tasks per user.
This is equivalent to a nested for loop. In the example below, we instantiate a
new :class:`.Pipeline` object `p`. Then for each user in 'Alice', 'Bob', and 'Dan'
we create new tasks for making the bed, doing laundry, and grocery shopping. In total,
we will have created 9 tasks that run in parallel as we did not define any dependencies
between the tasks.

.. image:: _static/images/dags/dags.007.png

.. code-block:: python

    >>> p = hp.Pipeline(name='nested-scatter-1')
    >>> for user in ['Alice', 'Bob', 'Dan']:
    ...     for chore in ['make-bed', 'laundry', 'grocery-shop']:
    ...         t = p.new_task(name=f'{user}-{chore}')
    ...         t.command(f'echo "user {user} is doing chore {chore}"')
    >>> p.run()


We can implement the same example as above with a function that implements the inner
for loop. The `do_chores` function takes a :class:`.Pipeline` object to add new tasks
to and a user name for whom to create chore tasks for. Like above, we create 9 independent
tasks. However, by structuring the code into smaller functions that take pipeline objects,
we can create more complicated dependency graphs and reuse components across various pipelines.


.. code-block:: python

    >>> def do_chores(p, user):
    ...     for chore in ['make-bed', 'laundry', 'grocery-shop']:
    ...         t = p.new_task(name=f'{user}-{chore}')
    ...         t.command(f'echo "user {user} is doing chore {chore}"')

    >>> p = hp.Pipeline(name='nested-scatter-2')
    >>> for user in ['Alice', 'Bob', 'Dan']:
    ...     do_chores(p, user)
    >>> p.run()

Lastly, we provide an example of a more complicated pipeline that has an initial
task, then scatters tasks per user, then has a series of gather / sink tasks
to wait for the per user tasks to be done before completing the pipeline.

.. image:: _static/images/dags/dags.008.png

.. code-block:: python

    >>> def do_chores(p, head, user):
    ...     chores = []
    ...     for chore in ['make-bed', 'laundry', 'grocery-shop']:
    ...         t = p.new_task(name=f'{user}-{chore}')
    ...         t.command(f'echo "user {user} is doing chore {chore}"')
    ...         t.depends_on(head)
    ...         chores.append(t)
    ...     sink = p.new_task(name=f'{user}-sink')
    ...     sink.depends_on(*chores)
    ...     return sink

    >>> p = hp.Pipeline(name='nested-scatter-3')
    >>> head = p.new_task(name='head')
    >>> user_sinks = []
    >>> for user in ['Alice', 'Bob', 'Dan']:
    ...     user_sink = do_chores(p, head, user)
    ...     user_sinks.append(user_sink)
    >>> final_sink = p.new_task(name='final-sink')
    >>> final_sink.depends_on(*user_sinks)
    >>> p.run()

.. _input-files:

Input Files
-----------

Previously, we discussed that :class:`.TaskResourceFile` are temporary files
and are created from :class:`.Task` objects. However, in order to read a file
that was not generated by the pipeline (input file), we use the method
:class:`.Pipeline.read_input` to create an :class:`.InputResourceFile`. An
input resource file can be used exactly in the same way as :class:`.TaskResourceFile`.
We can refer to an input resource file in a command using an f-string. In the example
below, we add the file `data/hello.txt` as an input resource file called `input`. We then
print the contents of `input` to stdout in :class:`.Task` `t`.

.. code-block:: python

    >>> p = hp.Pipeline(name='hello-input')
    >>> input = p.read_input('data/hello.txt')
    >>> t = p.new_task(name='hello')
    >>> t.command(f'cat {input}')
    >>> p.run()

Why do we need to explicitly add input files to pipelines rather than referring
directly to the path in the command? You could refer directly to the path when using the
:class:`.LocalBackend`, but only if you are not specifying a docker image to use when running
the command with :meth:`.Task.image`. This is because Pipeline copies any input files to a special
temporary directory which gets mounted to the Docker container. When using the :class:`.BatchBackend`,
input files would primarily be files in Google Storage. Many commands do not know how to handle file
paths in Google Storage. Therefore, we suggest explicitly adding all input files as input resource
files to the pipeline so to make sure the same code can run in all scenarios.


Output Files
------------

All files generated by Pipeline are temporary files! They are copied as appropriate between tasks
for downstream tasks' use, but will be removed when the pipeline has terminated. In order to save
files generated by a pipeline for future use, you need to explicitly call :meth:`.Pipeline.write_output`.
The first argument to :meth:`.Pipeline.write_output` can be any type of :class:`.ResourceFile` which includes input resource
files and task resource files as well as resource groups as described below. The second argument to write_output
should be a local file path when using the :class:`.LocalBackend` and a google storage file path when using
the :class:`.BatchBackend`.


.. code-block:: python

    >>> p = hp.Pipeline(name='hello-input')
    >>> t = p.new_task(name='hello')
    >>> t.command(f'echo "hello" > {t.ofile}')
    >>> p.write_output(t.ofile, 'output/hello.txt')
    >>> p.run()


Resource Groups
---------------

Many bioinformatics tools treat files as a group with a common file
path and specific file extensions. For example, `PLINK <https://www.cog-genomics.org/plink/>`_
stores genetic data in three files: `*.bed` has the genotype data,
`*.bim` has the variant information, and `*.fam` has the sample information.
PLINK can take as an input the path to the files expecting there will be three
files with the appropriate extensions. It also writes files with a common file root and
specific file extensions including when writing out a new dataset or outputting summary statistics.

To enable Pipeline to work with file groups, we added a :class:`.ResourceGroup` object
that is essentially a dictionary from file extension name to file path. When creating
a :class:`.ResourceGroup` in a :class:`.Task` (equivalent to a :class:`.TaskResourceFile`),
you first need to use the method :meth:`.Task.declare_resource_group` to declare the files
in the resource group explicitly before referring to the resource group in a command.
This is because the default when referring to an attribute on a task that has not been defined
before is to create a :class:`.TaskResourceFile` and not a resource group.

In the example below, we first declare that `create.bfile` will be a resource group with three files.
The attribute name comes from the name of the key word argument `bfile`. The constructor expects
a dictionary as the value for the key word argument. The dictionary defines the names of each of the files
and the file path where they should be located. In this example, the file paths contain
`{root}` which is the common temporary file path that will get substituted in to create the
final file path. Do not use f-strings here as we substitute a value for `{root}` when creating
the resource group!

We can then refer to `create.bfile` in commands which gets interpolated with the common temporary file root path
(equivalent to `{root}`) or we can refer to a specific file in the resource group such as `create.bfile.fam`.

.. code-block:: python

    >>> p = hp.Pipeline(name='resource-groups')
    >>> create = p.new_task(name='create-dummy')
    >>> create.declare_resource_group(bfile={'bed': '{root}.bed',
    ...                                      'bim': '{root}.bim',
    ...                                      'fam': '{root}.fam'})
    >>> create.command(f'plink --dummy 10 100 --make-bed --out {create.bfile}')
    >>> p.run() # doctest: +SKIP


As described previously for :ref:`input files <input-files>`, we need a
separate mechanism for creating a resource group from a set of input files
using the method :meth:`.Pipeline.read_input_group`. The constructor takes
key word arguments that define the name of the file such as `bed` to the path
where that file is located. The resource group is then a dictionary of the name
of the attribute to an :class:`.InputResourceFile`.

In the example below, we created an input resource group `bfile` with three files.
The group's common root file path can be referred to with `bfile` in a command or
you can reference a specific input file such as `bfile.fam`.

.. code-block:: python

    >>> p = hp.Pipeline(name='resource-groups')
    >>> bfile = p.read_input_group(bed='data/example.bed',
    ...                            bim='data/example.bim',
    ...                            fam='data/example.fam')
    >>> wc_bim = p.new_task(name='wc-bim')
    >>> wc_bim.command(f'wc -l {bfile.bim}')
    >>> wc_fam = p.new_task(name='wc-fam')
    >>> wc_fam.command(f'wc -l {bfile.fam}')
    >>> p.run()


If your tool requires a specific extension for the input files to have such
as the file is gzipped, then you'd need to create the resource group as follows:

.. code-block:: python

    >>> rg = p.read_input_group(**{'txt.gz': 'data/hello.txt.gz'})
    >>> rg['txt.gz']

Backends
--------

There are two backends that execute pipelines: the :class:`.LocalBackend` and the
:class:`.BatchBackend`. The local backend is used by default and executes tasks
on your local computer. The Batch backend executes tasks in a shared compute cluster
managed by the Hail team. To use the Batch Service, follow the directions :ref:`here <sec-batch-service>`.
